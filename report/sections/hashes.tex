
\section{Preliminaries}\label{sec:vanilla}
%---------------------------------------------------------------------------------------------------%
% Things to put here:
% - A bit of computational and complexity introduction
% - Hash functions
% - Trees (Merkle Tree and Augmented Binary Tree)
%---------------------------------------------------------------------------------------------------%
In this section we are going to introduce some fundamental concepts;
while some are relatively basilar, it can still be useful to skim over them to be sure of having
a firm grasp of the foundations of ZK-SNARK systems.
\subsection{Computational models and complexity}
A \emph{computational model} (or model of computation) is any kind of `device', either physical or
mathematical, which is able to compute algorithms to solve problems.
A particularly interesting class of problems are \emph{decision problems}, the ones that can be
answered with `yes' or `no'.
Every computational model is able to \emph{decide} only a subclass of all decision problems, and
even then, not all can be solved \emph{efficiently}, that is, by using an amount of resources
(tipically, time and space) which is upper-bounded by some polynomial over the input length.
Problems for which a polynomial bound doesen't exist or isn't known are said to be \emph{hard} for
the computational model.
For example, finding solutions to boolean equations (the \textsc{sat} problem) is believed to be
hard for deterministic Turing machines, but it is easy for non-deterministic ones.
Unfortunately, non-deterministic Turing machines (along with any other non-deterministic model of
computation) are more of a mathematical tool than anything, and there seems to be no practical way
to efficiently solve all of the problems which would take Non-deterministic Polynomial time
(NP-complete problems).
This is well expressed by the strong Church-Turing thesis: no \emph{physical} computational model
can be exponentially faster than deterministic Turing machines.
Of course, finding a model which is able to solve NP-complete problems would in fact mean that we
have found a way to implement non-determinism, which is largely belived to be impossible.
On the other hand, there are problems which lie in a `gray zone' between \textsc{NP-complete} and
\textsc{P}: they are believed to be hard for deterministic models (hence, they are not in \textsc{P}),
but there seems to be no way to show that they are \textsc{NP-complete}.
The most famous of such problems is factorization of natural numbers: Shor's algorithm is an
hybrid quantum algorithm that can factorize numbers in polynomial time.
While still far from usable in practical cases, its existence proves that one must be extremely
careful when talking about the hardness of problems, especially when applied to cryptography.

\subsection{Prime fields}
While computational models operate over binary strings, that is, elements of \({\{0, 1\}}^*\),
where \(*\) indicates Kleene's closure, in cryptography these strings are often interpreted as
elements of some algebraic structure, tipically \emph{fields}.
A field is any set equipped with two binary functions (operations), called field addition
(denoted \(\oplus \)) and field multiplication (denoted \(\otimes \)), which, in simple terms, have all
the nice properties of addition and multiplications over complex numbers (\(\mathbb{C}\) is a field,
where \(\oplus \equiv +\) and \(\otimes \equiv \times \)).
The most common field used to represent bits is of course the \emph{boolean field} \(\mathbb{B}\),
where \(\oplus \equiv \textsc{xor}\) and \(\otimes \equiv \textsc{and}\).
Strings of bits of length \(n\) can be interpreted as elements of a \emph{finite field} 
\(\mathbb{Z}_{q}\), where \(\oplus \) and \(\otimes \) are defined respectively as integer addition 
and multiplication modulo \(q\), with either \(q = 2^n\) or \(q\) being some prime number 
smaller than \(2^n\) (in this case, all bit strings representing integers in the interval 
\([q, 2^n-1]\) must be reduced modulo \(q\)).
When \(q\) is prime, 

\subsection{Hash functions}
Hash functions are a fundamental tool in many fields of computer science, and cryptography is
arguably the most prominent.
Formally, an hash function is any function \(H\colon {\{0, 1\}}^{*} \mapsto {\{0, 1\}}^n\), that
is any function which maps arbitrarly long \emph{messages} to fixed-size \emph{digests}.
From the definition, it is immediate to see that there are an infinite number of messages which map
to the same digest.
While an operation like truncation is a (very simple) hash function, in cryptography we are
interested in functions that provide additional guarantees: the assumption is that a digest sohuld
represent a message in a one-way fashion: while there are infinite messages which map to the same
digest, it must be hard to find them.
Ideally, a cryptographic hash function should behave like a perfect random function.
This is of course impossible, as the output of an hash function must only depend deterministically
on its input; the aim then is to build functions which are hard to distinguish from a random
function.
\begin{definition}[Cryptographic hash function]
	Given \(n \in \mathbb{N}\), an \(n\)(-bit) cryptographic hash function (CHF) is any function
	\(H\colon {\{0, 1\}}^{*} \mapsto {\{0, 1\}}^n\) which satisfies the following properties:
	\begin{itemize}
		\item \textbf{Collision resistance}: It is hard to find two messages \(m_1, m_2\) such
		      that \(H(m_1) = H(m_2)\).
		\item \textbf{Preimage resistance}: Given some digest \(h\), it is hard to find a
		      message \(m\) such that \(H(m) = h\) (\(H\) is a one-way function).
		\item \textbf{Second preimage resistance}: Given some message \(m_1\), it is hard to
		      find a message \(m_2\) such that \(H(m_1) = H(m_2)\).
	\end{itemize}
\end{definition}

\noindent While some of the requirements might seem redundant (for example, if it is hard for an
attacker to find a collision for chosen messages, it must be hard when one is fixed),
the difference usually lies in how exactly we define hardness for each property.
For collision resistance, an ideal CHF requires about \(2^{n/2}\) evaluations to find a collision
(birthday paradox), while for preimage resistance it would require about \(2^n\) evaluations.
Tipically, a CHF is built by applying some known secure constructions to functions which are
simpler to devise.
\begin{definition}[Pseudorandom keyed permutation]
	Given \(l, n \in \mathbb{N}\), an \(l/n\)(-bit) pseudorandom keyed permutation (PKP) is any
	bijective function:
	\[F\colon {\{0, 1\}}^l \times {\{0, 1\}}^n \mapsto {\{0, 1\}}^l\]
	which is hard to distinguish from an uniform random distribution.
\end{definition}

\noindent PKPs are often built by iterating a keyed permutation \(F\) for some number \(r\) of
rounds, since \(F\) by itself might be relatively easy to invert.
A block cipher is a pseudorandom keyed permutation which changes the key being used in each
round through a key-scheduling function.
Unkeyed permutations can be derived from keyed ones simply by fixing the key to some arbitrary
value.
\begin{definition}[One-way compression function]
	Given \(l_1, l_2, n \in \mathbb{N}\), an \(l_1/n/l_2\)(-bit) one-way compression function (OWCF)
	is any function:
	\[F\colon {\{0, 1\}}^{l_1} \times {\{0, 1\}}^n \mapsto {\{0, 1\}}^{l_2}\]
\end{definition}

\noindent There are many known ways to build OWCFs from pseudorandom keyed
permutations, and, in turn, CHFs from OWCFs.
We are going to use the Davies-Meyer and the Merkle-Damg\r{a}rd constructions respectively.
\begin{theorem}[Davies-Meyer construction]
	Given a \(l/n\) pseudorandom keyed permutation \(E\), some \(i, k \in \mathbb{N}\), some
	\(v \in {\{0, 1\}}^l\), and some \(m \in {\{0, 1\}}^{kn}\), then any function \(F_E\) such that:
	\begin{align*}
		 & F_{E,i}(v, m) =
		\begin{cases}
			E(v, m_{\range{1}{n}})                      & i = 1         \\
			E(F_{E, i-1}(v, m), m_{\range{i(n-1)}{in}}) & 2 \le i \le k \\
		\end{cases} \\
		 & F_E = F_{E, k}
	\end{align*}
	is a \(l/kn/l\) OWCF\@.
\end{theorem}
\begin{theorem}[Merkle-Damg\r{a}rd construction]
	Given a \(l_1/n/l_2\) OWCF \(F\), some \(k \in \mathbb{N}\), some \(v \in {\{0, 1\}}^{l_1}\),
	some \(m \in {\{0, 1\}}^*\) and some padding funcion:
	\[P(m)\colon {\{0, 1\}}^{|m|} \mapsto {\{0, 1\}}^{|m| + (-|m| \bmod n) + kn}\]
	such that, \(\forall m, m' \in {\{0, 1\}}^*\colon \)
	\[(|m| = |m'| \then |P(m)| = |P(m')|) \land (|m| \neq |m'| \then m_{|P(m)|} \neq m'_{|P(m')|})\]
	then any function \(H_F\) such that:
	\begin{align*}
		 & H_{F, i}(v, m) =
		\begin{cases}
			F(v, m_1)                & i = 1            \\
			F(H_{F, i-1}(v, m), m_i) & 1 < i \le |P(m)| \\
		\end{cases} \\
		 & H_F = H_{F, |P(m)|}
	\end{align*}
	is a cryptographic hash function.
\end{theorem}

\subsection{Tree hash modes}
An important application of CHFs is in \emph{prover-verifier games}:
for any message \(m\), the digest \(h = H(m)\), where \(H\) is an \(n\) CHF, can be
used as a \emph{binding commitment} for \(m\): a verifier is convinced that the prover knows \(m\)
simply by asking him to share \(h\), with overwhelming confidence (\(\approx 1 - \frac{1}{2^n}\)).

If the prover wants to commit to a list of \(k\) messages, a possibility would be to share with the
verifier the hash of every message: this would require a \(\BigO(k)\) communication cost and a
\(\BigO(k)\) verification cost.
A slightly better alternative would be for the prover to share \(H(\{m_1, \dots, m_k\})\): the
communication cost would only be \(\BigO(1)\), but verification would still cost \(\BigO(k)\).
\begin{definition}[Merkle Tree]
	Given some \(k \in \mathbb{N}\), a CHF \(H\) and a set of messages
	\(S = \{m_1, \dots, m_{s^{k-1}} \mid \forall i\colon m_i \in {\{0, 1\}}^*\} \),
	a Merkle Tree (MT) is a complete binary tree of height \(k\) such that:
	\begin{enumerate}
		\item The leaf nodes \(\nu_1, \dots, \nu_{2^{k-1}}\) contain \(H(m_1), \dots,
		      H(m_{2^{k-1}})\).
		\item Every other node \(\nu \) contains \(H(\nu_l, \nu_r)\), where \(\nu_l\) is the left
		      child of \(\nu \) and \(\nu_r\) is the right child of \(\nu \).
	\end{enumerate}
\end{definition}

\noindent By using Merkle trees, the prover only needs to send to the verifier, as a commitment for
some message \(m_i\) among \(k = 2^{\lfloor\log_2(k)\rfloor}\) messages, the contents of the
co-path from the leaf containing \(m_i\) to the root (plus the hash of \(m_i\)): this requires just
\(\BigO(\log_2(k))\) communication effort and \(\BigO(\log_2(k))\) verification effort.
Another advantage of Merkle trees is that bottom-up construction is very easy to parallelize,
and its usefulness can be appreciated even more when considering a scenario where different
messages actually belong to different provers.
\begin{definition}[Augmented Binary tRee]
	Given some \(k \in \mathbb{N}\), a CHF \(H\), and a set of messages
	\(S = \{m_1, \dots, m_{2^{k-1} + 2^{k-2}-1} \mid \forall i\colon m_i \in {\{0, 1\}}^*\} \),
	an Augmented Binary tRee (ABR) is a complete binary tree of
	height \(k\) augmented with \emph{middle} nodes, such that:
	\begin{enumerate}
		\item The leaf nodes \(\nu_{1}, \dots, \nu_{2^{k-1}}\) contain \(H(m_1), \dots,
		      H(m_{2^{k-1}})\).
		\item There are no middle nodes in the leaf layer.
		\item The middle nodes \(\nu_{2^{k-1}+1}, \dots, \nu_{|S|}\) contain
		      \(H(m_{2^{k-1}+1}), \dots, H(m_{|S|})\).
		\item Every other node \(\nu \) contains \(H(\nu_l \oplus \nu_m, \nu_r \oplus \nu_m)
		      \oplus \nu_r \), where \(\nu_l\) is the left child of \(\nu \), \(\nu_r\) is the right
		      child of \(\nu \), and \(\nu_m\) is the middle child of \(\nu \), or
		      \(0\) if \(\nu \) doesen't have a middle child.
	\end{enumerate}
\end{definition}

\noindent ABRs can store 50\% more messages than Merkle Trees for the same height, resulting in the
same number of calls to \(H\), at the cost of performing 3 additional \(\oplus \) operations for
every call (we assume that \(TIME(\oplus) \ll TIME(H)\)).
